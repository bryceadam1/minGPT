Loading Dataset
Dataset Loaded
Instantiating Model
number of parameters: 12.34M
Instantiating Trainer
running on device cuda
Starting Training
Batch: 0, Loss: 10.836250305175781
Batch: 5, Loss: 9.965069770812988
Batch: 10, Loss: 9.899985313415527
Batch: 15, Loss: 9.524333953857422
Batch: 20, Loss: 9.18835163116455
Batch: 25, Loss: 8.868857383728027
Batch: 30, Loss: 8.888657569885254
Batch: 35, Loss: 8.574482917785645
Batch: 40, Loss: 8.207752227783203
Batch: 45, Loss: 8.199975967407227
Batch: 50, Loss: 7.845845699310303
Batch: 55, Loss: 7.690526962280273
Batch: 60, Loss: 7.586748123168945
Batch: 65, Loss: 7.377457141876221
Batch: 70, Loss: 7.325135707855225
Batch: 75, Loss: 7.173544406890869
Batch: 80, Loss: 7.247398853302002
Batch: 85, Loss: 7.195245265960693
Batch: 90, Loss: 7.142411231994629
Batch: 95, Loss: 7.168370723724365
Batch: 100, Loss: 7.047192096710205
Batch: 105, Loss: 6.915842533111572
Batch: 110, Loss: 6.688220024108887
Batch: 115, Loss: 6.647730827331543
Batch: 120, Loss: 6.841444969177246
Batch: 125, Loss: 6.700572490692139
Batch: 130, Loss: 6.52726411819458
Batch: 135, Loss: 6.072758197784424
Batch: 140, Loss: 6.3244757652282715
Batch: 145, Loss: 6.597099781036377
Batch: 150, Loss: 6.801906585693359
Batch: 155, Loss: 6.554646015167236
Batch: 160, Loss: 6.117640972137451
Batch: 165, Loss: 6.435322284698486
Batch: 170, Loss: 6.192261219024658
Batch: 175, Loss: 6.390621662139893
Batch: 180, Loss: 6.326735019683838
Batch: 185, Loss: 5.997317314147949
Batch: 190, Loss: 6.301119804382324
Batch: 195, Loss: 6.1298112869262695
Batch: 200, Loss: 6.143957138061523
Batch: 205, Loss: 6.2974934577941895
Batch: 210, Loss: 6.179681777954102
Batch: 215, Loss: 5.790651798248291
Batch: 220, Loss: 6.018202304840088
Batch: 225, Loss: 6.147825241088867
Batch: 230, Loss: 5.857880115509033
Batch: 235, Loss: 5.848433017730713
Batch: 240, Loss: 6.326013565063477
Batch: 245, Loss: 5.9651689529418945
Batch: 250, Loss: 6.007376194000244
Batch: 255, Loss: 6.291898250579834
Batch: 260, Loss: 6.543305397033691
Batch: 265, Loss: 5.9441304206848145
Batch: 270, Loss: 5.952260971069336
Batch: 275, Loss: 6.2237772941589355
Batch: 280, Loss: 6.091280460357666
Batch: 285, Loss: 6.214879035949707
Batch: 290, Loss: 6.1895670890808105
Batch: 295, Loss: 6.055751800537109
Batch: 300, Loss: 6.096197128295898
Batch: 305, Loss: 6.304393291473389
Batch: 310, Loss: 5.4477996826171875
Batch: 315, Loss: 5.549393653869629
Batch: 320, Loss: 5.992199897766113
Batch: 325, Loss: 5.777708530426025
Batch: 330, Loss: 6.378178596496582
Batch: 335, Loss: 5.866582870483398
Batch: 340, Loss: 5.966814041137695
Batch: 345, Loss: 5.998071670532227
Batch: 350, Loss: 5.594643592834473
Batch: 355, Loss: 5.902398109436035
Batch: 360, Loss: 6.118488788604736
Batch: 365, Loss: 5.792702674865723
Batch: 370, Loss: 6.073854446411133
Batch: 375, Loss: 6.060620307922363
Batch: 380, Loss: 5.902986526489258
Batch: 385, Loss: 5.542135238647461
Batch: 390, Loss: 5.86383581161499
Batch: 395, Loss: 5.658785820007324
Batch: 400, Loss: 5.557257652282715
Batch: 405, Loss: 6.033848762512207
Batch: 410, Loss: 5.435230731964111
Batch: 415, Loss: 5.863380432128906
Batch: 420, Loss: 5.699543476104736
Batch: 425, Loss: 5.787359714508057
Batch: 430, Loss: 6.163172245025635
Batch: 435, Loss: 5.980266571044922
Batch: 440, Loss: 5.721810817718506
Batch: 445, Loss: 5.571990013122559
Batch: 450, Loss: 6.0441107749938965
Batch: 455, Loss: 5.753448963165283
Batch: 460, Loss: 5.62909460067749
Batch: 465, Loss: 5.818130016326904
Batch: 470, Loss: 5.678976058959961
Batch: 475, Loss: 5.618995666503906
Batch: 480, Loss: 5.665075302124023
Batch: 485, Loss: 5.871408939361572
Batch: 490, Loss: 5.927229404449463
Batch: 495, Loss: 5.55894136428833
Batch: 500, Loss: 5.7053632736206055
Batch: 505, Loss: 5.489161014556885
Batch: 510, Loss: 5.910550594329834
Batch: 515, Loss: 5.881121635437012
Batch: 520, Loss: 5.326082229614258
Batch: 525, Loss: 5.605574131011963
Batch: 530, Loss: 5.683432102203369
Batch: 535, Loss: 5.465378761291504
Batch: 540, Loss: 5.666762828826904
Batch: 545, Loss: 5.662763595581055
Batch: 550, Loss: 5.155451774597168
Batch: 555, Loss: 5.122418403625488
Batch: 560, Loss: 5.419477939605713
Batch: 565, Loss: 5.344226837158203
Batch: 570, Loss: 5.3392205238342285
Batch: 575, Loss: 5.838444232940674
Batch: 580, Loss: 5.7268452644348145
Batch: 585, Loss: 5.476109027862549
Batch: 590, Loss: 5.428618907928467
Batch: 595, Loss: 5.597858905792236
Batch: 600, Loss: 5.737860202789307
Batch: 605, Loss: 5.5559492111206055
Batch: 610, Loss: 5.769453525543213
Batch: 615, Loss: 5.5785298347473145
Batch: 620, Loss: 5.541930198669434
Batch: 625, Loss: 5.758886814117432
Batch: 630, Loss: 5.279129981994629
Batch: 635, Loss: 5.556031703948975
Batch: 640, Loss: 5.518803119659424
Batch: 645, Loss: 5.673148155212402
Batch: 650, Loss: 5.422353267669678
Batch: 655, Loss: 5.716518878936768
Batch: 660, Loss: 5.155470848083496
Batch: 665, Loss: 5.182817459106445
Batch: 670, Loss: 5.229061603546143
Batch: 675, Loss: 5.79105806350708
Batch: 680, Loss: 5.288617134094238
Batch: 685, Loss: 5.470743656158447
Batch: 690, Loss: 5.228924751281738
Batch: 695, Loss: 5.545105934143066
Batch: 700, Loss: 5.405907154083252
Batch: 705, Loss: 5.376621246337891
Batch: 710, Loss: 5.359428882598877
Batch: 715, Loss: 5.338241100311279
Batch: 720, Loss: 5.2590012550354
Batch: 725, Loss: 5.551584243774414
Batch: 730, Loss: 5.3794169425964355
Batch: 735, Loss: 5.401771068572998
Batch: 740, Loss: 5.466882705688477
Batch: 745, Loss: 5.170917987823486
Batch: 750, Loss: 5.168789863586426
Batch: 755, Loss: 5.500429630279541
Batch: 760, Loss: 5.252532005310059
Batch: 765, Loss: 5.719067096710205
Batch: 770, Loss: 5.830749988555908
Batch: 775, Loss: 5.691268444061279
Batch: 780, Loss: 4.99882173538208
Batch: 785, Loss: 5.883660793304443
Batch: 790, Loss: 5.520971775054932
Batch: 795, Loss: 5.414926528930664
Batch: 800, Loss: 5.205844402313232
Batch: 805, Loss: 5.315241813659668
Batch: 810, Loss: 5.695139408111572
Batch: 815, Loss: 5.179022312164307
Batch: 820, Loss: 5.654652118682861
Batch: 825, Loss: 5.199473857879639
Batch: 830, Loss: 5.6127610206604
Batch: 835, Loss: 5.700922966003418
Batch: 840, Loss: 5.300186634063721
Batch: 845, Loss: 5.357080936431885
Batch: 850, Loss: 5.2820563316345215
Batch: 855, Loss: 5.507580757141113
Batch: 860, Loss: 5.290192604064941
Batch: 865, Loss: 5.28131628036499
Batch: 870, Loss: 5.304559707641602
Batch: 875, Loss: 5.592120170593262
Batch: 880, Loss: 5.586775779724121
Batch: 885, Loss: 5.5108537673950195
Batch: 890, Loss: 5.258265972137451
